<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>Side Project</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<link href="css/style.css" rel="stylesheet" type="text/css" media="screen" />
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	  tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		processEscapes: true
	  }
	});
  </script>
  
  <script type="text/javascript"
	  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
	
	<!-- jsicons
    ================================================== -->
    <link rel="shortcut icon" href="images/github_logo.png" type="image/x-icon">
    <link rel="icon" href="images/github_logo.png" type="image/x-icon">
</head>
<body>

<!-- start header -->
<div id="header">
	<h1>Logistic Regression</h1>
</div>
<!-- end header -->
<!-- start page -->
<div id="page">
	<!-- start content -->
	<div id="content">
		<div class="post">
			<h2 id=likelihood class="title">Maximum Likelihood Estimation</h2>
			<p class="byline"><small>Retrieved via The Pennsylvania State University, Department of Statistics <a target="_blank" rel="noopener noreferrer" href="https://onlinecourses.science.psu.edu/stat414/node/191/">Online Programs</a> July 9th, 2018</small></p>
			<div class=entry>
				<p>
					&nbsp; &nbsp; Let a data set $\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n]$, 
					be a random sample from a distribution that depends on one or more unknown parameters 
					$\mathbf{\theta}$ with probability function $f(\mathbf{x}_i\mid \mathbf{\theta})$, the likelihood function is:
					$$L(\mathbf{\theta}) = \Pi f(\mathbf{x}_i\mid \mathbf{\theta})$$
				</p>
				<p>
					&nbsp; &nbsp; The equality uses the uppercase pi as the shorthand mathematical notation of a product of indexed terms. 
					Our primary goal here will be to find a point estimator $u(\mathbf{x}_1, \mathbf{x}_2,..., \mathbf{x}_n)$, 
					such that $u(\mathbf{x}_1, \mathbf{x}_2,..., \mathbf{x}_n)$ is a "good" point estimate of $\mathbf{\theta}$. 
					The value of $\mathbf{\theta}$ that maximizesthe likelihood of getting the data we observed would be reasonable that 
					a good estimate of the unknown parameter $\theta$. Because the "random sample" were independent,  
					we could use the joint probability mass (or density) function of $\mathbf{X}$, to implement the method in practice.
				</p>
				
				<img id=log_func src="./images/log_func.png">
				<p>
					&nbsp; &nbsp; In order to implement the method of maximum likelihood, 
					we need to find the p that maximizes the likelihood $L(\theta)$. We need to put on our calculus hats now, 
					since in order to maximize the function, we are going to need to differentiate the likelihood function with respect to $\theta$. 
					In doing so, we'll use a "trick" that often makes the differentiation a bit easier.  
					Note that the natural logarithm is an <strong><a target="_blank" rel="noopener noreferrer" href="http://mathworld.wolfram.com/StrictlyIncreasingFunction.html">Strictly Increasing Function</a></strong> of $x$.	
					
				</p>
				
				<p>&nbsp; &nbsp; The log-likelihood function is: 

				$$\text{log}L(\theta)=\sum \text{log}f(\mathbf{x}_i\mid \mathbf{\theta})$$</p>
				
			</div>
					
		</div>
		<div class="post">
			<h2 id=bayes class="title">Bayesâ€™ theorem</h2>
			<p class="byline"><small>Retrieved via <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/">wiki</a> July 9th, 2018</small></p>
			<div class=entry>
					<p>
						&nbsp; &nbsp; <strong><a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes' theorem</a></strong> is stated mathematically as the following equation:
						$$P(A\mid B) = \frac{P(B\mid A)P(A)}{P(B)}$$
						<br>where $A$ and $B$ are events and $P(B)\neq 0$.
						<br>&nbsp; &nbsp; $P(A\mid B)$ is a conditional probability: the likelihood of event $A$ occurring given that $B$ is true.
						<br>&nbsp; &nbsp; $P(B\mid A)$ is also a conditional probability: the likelihood of event $B$ occurring given that $A$ is true.
						<br>&nbsp; &nbsp; $P(A)$ and $P(B)$ are the probabilities of observing $A$ and $B$ B independently of each other.
					</p>
					<p>
						&nbsp; &nbsp; Driven:
						\begin{align}
						P(A\mid B) &= \frac{P(A\cap B)}{P(B)}\\
						P(B\mid A) &= \frac{P(B\cap A)}{P(A)}\\
						\end{align}
						\begin{align}
						&\because P(A\cap B) = P(B\cap A)\\
						&\therefore P(A\mid B)P(B) = P(B\mid A)P(A)
						\end{align}
						$$\Rightarrow P(A\mid B) = \frac{P(B\mid A)P(A)}{P(B)}$$
					</p>
			</div>
			
		</div>
		
		<div class="post">
			<h2 id=posteriori class="title">Posterior Probability</h2>
			<p class="byline"><small>Retrieved via <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/">wiki</a> July 16th, 2018</small></p>
			<div class=entry>
				<p>
					&nbsp; &nbsp; <strong><a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Posterior_probability">The posterior probability</a></strong> 
					is the probability of the parameters $\theta$  given the evidence $\mathbf{X}$: $p(\theta \mid \mathbf{X})$. 
					It contrasts with the likelihood function, which is the probability of the evidence given the parameters: $p(\mathbf{X} \mid \theta)$.
					The two are related as follows:
					<br>Let us have a prior belief that the probability distribution function is $p(\theta)$ and observations $\mathbf{X}$ with the likelihood $p(\mathbf{X} \mid \theta)$, then the posterior probability is defined as
					$$p(\theta \mid \mathbf{X}) = \frac {p(\mathbf{X} \mid \theta) p(\theta)}{p(\mathbf{X})}$$
				</p>
				<p>
					In logistic regression cass, the logistic regression model is:
					$$f(\mathbf{x}_i, y_i \mid \mathbf{w}) = \frac{1}{(1 + e^{-y_i \mathbf{w}^T \mathbf{x}_i})}$$
				</p>
				<p>
					The prior probability is:
					\begin{align}
						p(\mathbf{w}) &\sim \mathcal{N}(0, \lambda^{-1})\\
						&=\frac{1}{\sqrt{\frac{2\pi}{\lambda}}}e^{-\frac{\lambda(w - 0)^2}{2}}
					\end{align}
				</p>
				<p>
					The maximum maximum-a-posteriori is:
					\begin{align}
						P(\mathbf{w} \mid \mathbf{x}_i, y_i) &= \text{argmax }\frac{L(\mathbf{x}_i, y_i \mid \mathbf{w})p(\mathbf{w})}{p(\mathbf{x}_i, y_i)}\\
						&= \text{argmax }L(\mathbf{x}_i, y_i \mid \mathbf{w})p(\mathbf{w})\\
						&= \text{argmax log}L(\mathbf{x}_i, y_i \mid \mathbf{w}) + \text{log}p(\mathbf{w})\\
						&=\text{argmax }\sum\text{log}(1 + e^{-y_i \mathbf{w}^T \mathbf{x}_i})^{-1} + \text{log}\frac{1}{\sqrt{\frac{2\pi}{\lambda}}}e^{-\frac{\lambda(w - 0)^2}{2}}\\
						&=\text{argmax }-\sum\text{log}(1 + e^{-y_i \mathbf{w}^T \mathbf{x}_i}) - \frac{\lambda}{2}\mathbf{w}^T\mathbf{w}
					\end{align}
				</p>
			</div>
		</div>
	</div>
	<!-- end content -->
	<!-- start sidebar -->
	<div id="sidebar">
		<ul>
			<li>
				<h2>Topics</h2>
				<ul>
					<li><a href="#" data-toggle=collapse data-target="#introduct">What's Logistic Regression (3)</a></a></li>
					<div id=introduct class=collapse>
						<li><a href="introduction.html#intro">&nbsp; &nbsp; &nbsp; &nbsp; Introduction</a></li>
						<li><a href="introduction.html#definition">&nbsp; &nbsp; &nbsp; &nbsp; Definition of the logistic function</a></li>
						<li><a href="introduction.html#odds">&nbsp; &nbsp; &nbsp; &nbsp; Definition of the odds</a></li>
					</div>		
			
					<li><a href="#" data-toggle=collapse data-target="#result">Java Result versus Scikit-learn (3)</a></li>
					<div id=result class=collapse>
						<li><a href="result.html#bridge">&nbsp; &nbsp; &nbsp; &nbsp; Bridge to Java</a></li>
						<li><a href="result.html#titanic">&nbsp; &nbsp; &nbsp; &nbsp; Test Data - titanic</a></li>
						<li><a href="result.html#binary">&nbsp; &nbsp; &nbsp; &nbsp; Test Data - binary</a></li>
					</div>
					
					<li><a href="#" data-toggle=collapse data-target="#alg">Algorithm for Logistic Regression (3)</a></li>
					<div id="alg" class=collapse>
						<li><a href="jupyter-notebook.html#sigm">&nbsp; &nbsp; &nbsp; &nbsp; Sigmoid Function</a></li>
						<li><a href="jupyter-notebook.html#prior">&nbsp; &nbsp; &nbsp; &nbsp; Likelihood & Prior Function</a></li>
						<li><a href="jupyter-notebook.html#nt">&nbsp; &nbsp; &nbsp; &nbsp; Newton Step</a></li>
					</div>
					
					<li><a href="class_diagram_current.html">Class Diagram</a></li>
									
					<li><a href="#" data-toggle=collapse data-target="#tech">Technique for Building Algorithm (6)</a></li>
					<div id=tech class=collapse>
						<li><a href="technique.html#matrix">&nbsp; &nbsp; &nbsp; &nbsp; Matrix</a></li>
						<li><a href="technique.html#equation">&nbsp; &nbsp; &nbsp; &nbsp; Equation</a></li>
						<li><a href="technique.html#algorithm">&nbsp; &nbsp; &nbsp; &nbsp; Algorithm</a></li>
						<li><a href="technique.html#io">&nbsp; &nbsp; &nbsp; &nbsp; Reading & Writing File</a></li>
						<li><a href="technique.html#export">&nbsp; &nbsp; &nbsp; &nbsp; Export Runable .jar File</a></li>
						<li><a href="technique.html#summary">&nbsp; &nbsp; &nbsp; &nbsp; Summary</a></li>
					</div>
				</ul>
			</li>
			<li>
				<h2>Appendix</h2>
				<ul>
					<li><a href="logistic.html">Logistic Regression</a></li>
					<li><a href="#" data-toggle="collapse" data-target="#map">Maximum-a-Posteriori (3)</a></li>
					<div id="map" class=collapse>
						<li><a href="map.html#likelihood">&nbsp; &nbsp; &nbsp; &nbsp; Maximum Likelihood Estimation</a></li>
						<li><a href="map.html#bayes">&nbsp; &nbsp; &nbsp; &nbsp; Bayesâ€™ theorem</a></li>
						<li><a href="map.html#posteriori">&nbsp; &nbsp; &nbsp; &nbsp; Posterior Probability</a></li>
					</div>
					<li><a href="newton.html">Newton's Method</a></li>
				</ul>
			</li>
		</ul>
	</div>
	<!-- end sidebar -->
</div>

<!-- end page -->
<div id="footer">
	<p>&copy; 2018 All Rights Reserved &nbsp;&bull;&nbsp; Edited by <a target="_blank" rel="noopener noreferrer" href="http://github.com/Berchem">Berchem Lin</a>.</p>
</div>
</body>
<script type="text/javascript"
	  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>$("#map").collapse("show")</script>
</html>
